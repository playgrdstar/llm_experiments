{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to C:\\Users\\playgrdstar\\.cache\\huggingface\\token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os, requests, git, shutil\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.llms import HuggingFaceEndpoint\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "\n",
    "\n",
    "embedding_model_list = ['sentence-transformers/all-MiniLM-L6-v2', 'BAAI/bge-small-en-v1.5', 'BAAI/bge-large-en-v1.5']\n",
    "inference_model_list = ['google/gemma-2b-it', 'google/gemma-7b-it', 'mistralai/Mixtral-8x7B-Instruct-v0.1']\n",
    "\n",
    "HF_READ_API_KEY = os.environ[\"HF_READ_API_KEY\"]\n",
    "\n",
    "# Helper function for extracting text from Langchain's Document object\n",
    "def get_text(docs):\n",
    "    return [d.page_content for d in docs]\n",
    "\n",
    "inference_model_name = inference_model_list[2]\n",
    "\n",
    "model_parameters = {'max_new_tokens': 512, 'temperature': 0.3, 'top_p': 0.95}\n",
    "\n",
    "llm_model = HuggingFaceEndpoint(\n",
    "            repo_id=inference_model_name,                     \n",
    "            max_new_tokens=model_parameters['max_new_tokens'], \n",
    "            temperature=model_parameters['temperature'], \n",
    "            top_p=model_parameters['top_p'],\n",
    "            huggingfacehub_api_token=HF_READ_API_KEY\n",
    "        )\n",
    "\n",
    "hf = HuggingFaceEmbeddings(model_name=embedding_model_list[0])\n",
    "\n",
    "chunking_parameters = {'chunk_size': 512, 'chunk_overlap': 128}\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunking_parameters['chunk_size'], chunk_overlap=chunking_parameters['chunk_overlap'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = PyMuPDFLoader(\"https://www.berkshirehathaway.com/letters/2023ltr.pdf\")\n",
    "docs = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \" What investments are good?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Simple Vectorstore Retriever\n",
    "## To get a standard retriever from a vectorstore, just need to get VectorStoreRetriever from the as_retriever method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db_from_docs = Chroma.from_documents(texts, hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['at a premium. \\nThe lesson from Coke and AMEX? When you find a truly wonderful business, stick with \\nit. Patience pays, and one wonderful business can offset the many mediocre decisions that \\nare inevitable. \\n* * * * * * * * * * * * \\nThis year, I would like to describe two other investments that we expect to maintain \\nindefinitely. Like Coke and AMEX, these commitments are not huge relative to our resources. \\nThey are worthwhile, however, and we were able to increase both positions during 2023.', 'think to predict which will be the winners and losers. And those who tell you they know the answer \\nare usually either self-delusional or snake-oil salesmen. \\nAt Berkshire, we particularly favor the rare enterprise that can deploy additional capital at \\nhigh returns in the future. Owning only one of these companies – and simply sitting tight – can \\ndeliver wealth almost beyond measure. Even heirs to such a holding can – ugh! – sometimes live \\na lifetime of leisure.', 'is now. \\nOur Not-So-Secret Weapon \\nOccasionally, markets and/or the economy will cause stocks and bonds of some large and \\nfundamentally good businesses to be strikingly mispriced. Indeed, markets can – and \\nwill – unpredictably seize up or even vanish as they did for four months in 1914 and for a few days \\nin 2001. If you believe that American investors are now more stable than in the past, think back to \\nSeptember 2008. Speed of communication and the wonders of technology facilitate instant', 'Occasionally, the scene turns ugly. The politicians then become enraged; the most flagrant \\nperpetrators of misdeeds slip away, rich and unpunished; and your friend next door becomes \\nbewildered, poorer and sometimes vengeful. Money, he learns, has trumped morality. \\nOne investment rule at Berkshire has not and will not change: Never risk permanent loss \\nof capital. Thanks to the American tailwind and the power of compound interest, the arena in which', 'I can’t remember a period since March 11, 1942 – the date of my first stock purchase – that \\nI have not had a majority of my net worth in equities, U.S.-based equities. And so far, so good. \\nThe Dow Jones Industrial Average fell below 100 on that fateful day in 1942 when I “pulled the \\ntrigger.” I was down about $5 by the time school was out. Soon, things turned around and now that \\nindex hovers around 38,000. America has been a terrific country for investors. All they have']\n"
     ]
    }
   ],
   "source": [
    "simple_retriever = vector_db_from_docs.as_retriever(search_kwargs={\"k\": 5})\n",
    "retrieved_docs = simple_retriever.get_relevant_documents(query)\n",
    "print(get_text(retrieved_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Parent Document Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "parent_text_splitter = child_text_splitter = text_splitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = Chroma(collection_name=\"parent_child\", embedding_function=hf)\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()\n",
    "\n",
    "pr_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vector_db,\n",
    "    docstore=store,\n",
    "    child_splitter=child_text_splitter,\n",
    "    parent_splitter=parent_text_splitter,\n",
    ")\n",
    "pr_retriever.add_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"But before we get into VAR, we first need to discuss what value we are assessing risk against. What we want to measure would be the change in market prices over a time period (e.g. day to day). So what VAR would then tell us then would be how much we could lose (or gain) due to the change in prices. It's quite common to use lognormal instead of normal returns when computing the change in prices.\\n\\nUseful links which provide more information on the difference between the two -\", '---\\nlayout: default\\ntitle:  Measuring Market Risk in Python\\ndescription: Measuring Market Risk in Python\\ndate:   2021-01-26 00:00:00 +0000\\npermalink: /market_risk/\\ncategory: Finance\\n---\\n## Measuring Market Risk in Python\\n\\nVAR is a common term that one would usually come across in finance when it comes to the measurement of market risks.\\n\\nVAR, or Value At Risk, is basically a measure of the potential losses that one could face, at a specific level of confidence - e.g. 99%.', 'If you recall the basics of the [notebook][1] where we provided an introduction on market risk measures and VAR, you will recall that parametric VAR simply assumes a distribution and uses the first two moments (mean and standard deviation) to compute the VAR; whereas for historical VAR, you use the actual historical data and use the specific datapoint (or interpolated values between 2 datapoints) for the confidence level.', 'VAR can also be computed via simulation. Which is a good way to provide a quick introduction to Monte Carlo simulation.\\n\\nSimulated VAR at its core is quite simple. You basically take the moments (say mean and standard deviation if you assume a normal distribution), generate a simulated set of data with Monte Carlo simulation, and then get the required percentile. What this means is that we could also assume a non-normal distribution, say a t-distribution, and use that for simulation and to compute VAR.']\n"
     ]
    }
   ],
   "source": [
    "retrieved_docs = pr_retriever.get_relevant_documents(query)\n",
    "print(get_text(retrieved_docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multi_query_retriever(query, llm, retriever):\n",
    "    DEFAULT_QUERY_PROMPT = PromptTemplate(\n",
    "        input_variables=[\"question\"],\n",
    "        template=\"\"\"You are an AI language model assistant. Your task is \n",
    "        to generate 3 different versions of the given user \n",
    "        question to retrieve relevant documents from a vector  database. \n",
    "        By generating multiple perspectives on the user question, \n",
    "        your goal is to help the user overcome some of the limitations \n",
    "        of distance-based similarity search. Provide these alternative \n",
    "        questions separated by newlines. Original question: {question}\"\"\",\n",
    "    )\n",
    "    mq_llm_chain = LLMChain(llm=llm, prompt=DEFAULT_QUERY_PROMPT)\n",
    "    \n",
    "    generated_queries = parse_lines(mq_llm_chain.invoke(query)['text'])\n",
    "    all_queries = [query] + generated_queries\n",
    "    \n",
    "    all_retrieved_docs = []\n",
    "    for q in all_queries:\n",
    "        retrieved_docs = retriever.get_relevant_documents(q)\n",
    "        all_retrieved_docs.extend(retrieved_docs)\n",
    "    \n",
    "    unique_retrieved_docs = [doc for i, doc in enumerate(all_retrieved_docs) if doc not in all_retrieved_docs[:i]]\n",
    "    \n",
    "    return get_text(unique_retrieved_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['---\\nlayout: default\\ntitle:  How normal are you? Checking distributional assumptions.\\ndescription: How normal are you? Checking distributional assumptions.\\ndate:   2021-01-27 00:00:00 +0000\\npermalink: /normality_distribution_test/\\ncategory: Finance\\n---\\n## How normal are you? Checking distributional assumptions.\\n\\nThe need to understand the underlying distribution of data is critical in most parts of quantitative finance. Statistical tests can be applied for this purpose.',\n",
       " '[1]:\\thttps://medium.com/quaintitative/data-exploration-in-pandas-f7cd1a3b3594\\n[2]:\\thttps://medium.com/quaintitative/quickstart-to-visualising-and-analysing-financial-data-with-pandas-bbd835c9c560\\n[3]:\\thttps://playgrdstar.github.io/portfolio_optimisation_with_tensorflow/\\n[4]:\\thttps://medium.com/creative-coding-space/3-days-of-hand-coding-visualisations-introduction-64da30d8793f\\n[5]:\\thttps://github.com/playgrdstar/portfolio_optimisation_with_tensorflow',\n",
       " \"And since I will almost certainly forget what went through my mind during this process, I decided to jot down some notes on key points that I learnt or went through my mind. \\n\\nI'm not very hopeful about selling any of my NFTs, given how crowded the marketplaces have become in the short span of a year or so, so this post would probably be my main takeaway.\\n\\nSome of these points could potentially be wrong, or outdated. If so, please let me know.\",\n",
       " '- Dec. 2021: \"Learning User Interface Semantics from Heterogeneous Networks with Multimodal and Positional Attributes\" accepted by 27th ACM International Conference on Intelligent User Interfaces (IUI 2022)\\n- Oct. 2021: \"Learning Knowledge-Enriched Company Embeddings for Investment Management\" accepted by the 2nd ACM International Conference on AI in Finance (ICAIF 2021)',\n",
       " '---\\nlayout: default\\ntitle:  Monte Carlo Simulation of Value at Risk in Python\\ndescription: Monte Carlo Simulation of Value at Risk in Python\\ndate:   2021-01-26 00:00:02 +0000\\npermalink: /monte_carlo_var/\\ncategory: Finance\\n---\\n## Monte Carlo Simulation of Value at Risk in Python',\n",
       " 'information required by Section 3(a)(1)(A) to the extent\\n          reasonably practicable.',\n",
       " 'Section 1 -- Definitions.',\n",
       " 'b. Any arrangements, understandings, or agreements regarding the\\n     Licensed Material not stated herein are separate from and\\n     independent of the terms and conditions of this Public License.\\n\\n\\nSection 8 -- Interpretation.',\n",
       " 'c. indicate the Licensed Material is licensed under this\\n               Public License, and include the text of, or the URI or\\n               hyperlink to, this Public License.',\n",
       " '---\\nlayout: default\\ntitle: Some notes relating to Python\\ndescription: Some notes relating to Python\\ndate:   2021-02-10 00:00:00 +0000\\npermalink: /python_notes/\\ncategory: Notes\\n\\n---\\n## Some notes relating to Python\\n\\nThis is just an ongoing list of short notes or gists on general Python related stuff.\\n\\n- [Walk-through Python][3]\\n- [Introduction to Python data structures][5]\\n- [Introduction to arrays in Python][6]\\n- [Introduction to Pandas in Python][7]\\n- [Introduction to Data Munging in Python][12]\\n- [Introduction to Visualisation in Python][8]\\n- [Introduction to Data Exploration in Python][9]\\n- [Introduction to Simple Sample Data Generation in Python][13]\\n- [Introduction to Jupyter notebooks][4]\\n- [Extracting data from Twitter][1]\\n- [Downloading PDFs with Python][2]\\n- [Data munging with time-series datasets][10]\\n- [Introduction to Interpolation][18]\\n- [Introduction to Financial Data Exploration in Python][15]\\n- [Introduction to RSS data ingestion][11]\\n- [Getting data from Quandl in Python][14]\\n- [Introduction to Twitter Sentiment Analysis in Python][19]\\n- [Introduction to Jupyter notebook in the Cloud][16]\\n- [Introduction to Selenium][17]\\n- [Introduction to AWS EC2][20]\\n\\n[1]:\\thttps://medium.com/quaintitative/twitter-data-cc4f60f968f2\\n[2]:\\thttps://medium.com/quaintitative/pdf-downloads-b6b05bec366e\\n[3]:\\thttps://medium.com/quaintitative/a-walk-through-python-a-cheat-sheet-of-sorts-ea3c9f158cf8\\n[4]:\\thttps://medium.com/quaintitative/a-very-quick-introduction-to-jupyter-notebooks-eebb4695d2ce\\n[5]:\\thttps://medium.com/quaintitative/a-walk-through-python-data-structures-a78ee6355365\\n[6]:\\thttps://medium.com/quaintitative/arrays-through-numpy-7dea558884f3\\n[7]:\\thttps://medium.com/quaintitative/introduction-to-pandas-8742f05ab78\\n[8]:\\thttps://medium.com/quaintitative/loading-data-and-visualisation-with-pandas-3c71033b99b9\\n[9]:\\thttps://medium.com/quaintitative/data-exploration-in-pandas-f7cd1a3b3594\\n[10]:\\thttps://medium.com/quaintitative/time-series-analysis-with-python-958956c9a9f0\\n[11]:   https://medium.com/quaintitative/rss-ingestion-615da65515f2\\n[12]:   https://medium.com/quaintitative/data-munging-scale-transform-clean-in-python-9e073098fab7\\n[13]:   https://medium.com/quaintitative/super-simple-guide-to-generating-datasets-for-data-analysis-and-experimentation-14775ab37a1b\\n[14]:   https://medium.com/quaintitative/getting-data-from-quandl-6256beb86e92\\n[15]:   https://medium.com/quaintitative/quickstart-to-visualising-and-analysing-financial-data-with-pandas-bbd835c9c560\\\\\\n[16]:   https://medium.com/quaintitative/jupyter-notebook-in-the-cloud-70cbf9c2cd92\\n[17]:   https://medium.com/quaintitative/simple-selenium-c30b425fa129\\n[18]:   https://medium.com/quaintitative/super-simple-primer-on-interpolation-b05657ea95e9\\n[19]:   https://medium.com/quaintitative/twitter-sentiment-analysis-in-just-2-steps-2c76cd961e60\\n[20]:   https://medium.com/quaintitative/using-amazon-web-services-ec2-for-the-first-time-100a662ab51b']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_query_retriever(query, llm_model, simple_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Contextual Compression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.document_compressors import LLMChainExtractor, LLMChainFilter, EmbeddingsFilter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain_extractor = LLMChainExtractor.from_llm(llm_model)\n",
    "filter_extractor = LLMChainFilter.from_llm(llm_model)\n",
    "embeddings_filter = EmbeddingsFilter(embeddings=hf, similarity_threshold=0.75)\n",
    "retrieved_docs = simple_retriever.get_relevant_documents(query)\n",
    "compressed_docs = chain_extractor.compress_documents(retrieved_docs, query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compressed_retrieval(query, llm, retriever, extractor_type='chain', embedding_model=None):\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "    if extractor_type == 'chain':\n",
    "        extractor = LLMChainExtractor.from_llm(llm)\n",
    "    elif extractor_type == 'filter':\n",
    "        extractor = LLMChainFilter.from_llm(llm)\n",
    "    elif extractor_type == 'embeddings':\n",
    "        if hf is None:\n",
    "            raise ValueError(\"hf (embeddings model) must be provided for the embeddings extractor.\")\n",
    "        extractor = EmbeddingsFilter(embeddings=hf, similarity_threshold=0.75)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid extractor_type. Options are 'chain', 'filter', or 'embeddings'.\")\n",
    "    compressed_docs = extractor.compress_documents(retrieved_docs, query)\n",
    "\n",
    "    return get_text(compressed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['- The need to understand the underlying distribution of data is critical in most parts of quantitative finance.',\n",
       " '[2]:\\thttps://medium.com/quaintitative/quickstart-to-visualising-and-analysing-financial-data-with-pandas-bbd835c9c560\\n[5]:\\thttps://github.com/playgrdstar/portfolio_optimisation_with_tensorflow',\n",
       " 'None of the context is relevant to the question.',\n",
       " '- Oct. 2021: \"Learning Knowledge-Enriched Company Embeddings for Investment Management\" accepted by the 2nd ACM International Conference on AI in Finance (ICAIF 2021)\\n\\nReasoning: The context mentions a paper about \"Learning Knowledge-Enriched Company Embeddings for Investment Management\" which is relevant to finance as it is about investment management.',\n",
       " 'Finance\\nMonte Carlo Simulation of Value at Risk in Python\\n\\nReturn:\\nFinance\\nMonte Carlo Simulation of Value at Risk in Python']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compressed_retrieval(query, llm_model, simple_retriever, extractor_type='chain')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ensemble Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import BM25Retriever\n",
    "# combine with the simple_retriever, pr_retriever, and mq_retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "all_retrievers = [simple_retriever, pr_retriever, bm25_retriever]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_retriever(query, retrievers_list, c=60):\n",
    "\n",
    "    retrieved_docs_by_retriever = [retriever.get_relevant_documents(query) for retriever in all_retrievers]\n",
    "    weights = [1 / len(all_retrievers)] * len(all_retrievers)\n",
    "\n",
    "    # Calculate RRF scores for all documents\n",
    "    rrf_score = defaultdict(float)\n",
    "    for doc_list, weight in zip(retrieved_docs_by_retriever, weights):\n",
    "        for rank, doc in enumerate(doc_list, start=1):\n",
    "            rrf_score[doc.page_content] += weight / (rank + c)\n",
    "\n",
    "    # Chain all document lists into a single iterable\n",
    "    all_docs = chain.from_iterable(retrieved_docs_by_retriever)\n",
    "\n",
    "    # Define function to yield unique documents based on a key\n",
    "    def unique_by_key(iterable, key_func):\n",
    "        seen = set()\n",
    "        for element in iterable:\n",
    "            key = key_func(element)\n",
    "            if key not in seen:\n",
    "                seen.add(key)\n",
    "                yield element\n",
    "\n",
    "    # Sort documents by RRF score in descending order\n",
    "    sorted_docs = sorted(\n",
    "        unique_by_key(all_docs, lambda doc: doc.page_content),\n",
    "        key=lambda doc: rrf_score[doc.page_content],\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    return get_text(sorted_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['---\\nlayout: default\\ntitle:  How normal are you? Checking distributional assumptions.\\ndescription: How normal are you? Checking distributional assumptions.\\ndate:   2021-01-27 00:00:00 +0000\\npermalink: /normality_distribution_test/\\ncategory: Finance\\n---\\n## How normal are you? Checking distributional assumptions.\\n\\nThe need to understand the underlying distribution of data is critical in most parts of quantitative finance. Statistical tests can be applied for this purpose.',\n",
       " '[1]:\\thttps://medium.com/quaintitative/data-exploration-in-pandas-f7cd1a3b3594\\n[2]:\\thttps://medium.com/quaintitative/quickstart-to-visualising-and-analysing-financial-data-with-pandas-bbd835c9c560\\n[3]:\\thttps://playgrdstar.github.io/portfolio_optimisation_with_tensorflow/\\n[4]:\\thttps://medium.com/creative-coding-space/3-days-of-hand-coding-visualisations-introduction-64da30d8793f\\n[5]:\\thttps://github.com/playgrdstar/portfolio_optimisation_with_tensorflow',\n",
       " '- Dec. 2021: \"Learning User Interface Semantics from Heterogeneous Networks with Multimodal and Positional Attributes\" accepted by 27th ACM International Conference on Intelligent User Interfaces (IUI 2022)\\n- Oct. 2021: \"Learning Knowledge-Enriched Company Embeddings for Investment Management\" accepted by the 2nd ACM International Conference on AI in Finance (ICAIF 2021)',\n",
       " '---\\nlayout: default\\ntitle:  Monte Carlo Simulation of Value at Risk in Python\\ndescription: Monte Carlo Simulation of Value at Risk in Python\\ndate:   2021-01-26 00:00:02 +0000\\npermalink: /monte_carlo_var/\\ncategory: Finance\\n---\\n## Monte Carlo Simulation of Value at Risk in Python',\n",
       " \"### Sell ###\\n\\nOnce you are happy with what you have listed. You can sell them. This is when gas fees actually take effect. First you select the NFT you want to sell.\\n\\nSelect the _Sell_ button on the top-right. You can choose between a **fixed price sale**, a typical **auction** (where the seller bids up), or a **Dutch auction** (where the price declines over time til someone purchases the item). For all 3, you can select the duration of the sale, up to 6 months. The advice typically given is not to set the duration for too long, as you might change your mind, and it is better to let it expire and re-list, then to incur costs cancelling the sale. I'm not too sure about the costs involved in cancellations though. \\n\\nAlso note that unless you want the buyer to buy multiple pieces at a go, you should set the quantity to 1. Setting it to, say 2, means the buyer would have to purchase both editions of the NFT at once. Not an issue if you are just listing and selling one of a kind NFTs.\\n\\nWhen you complete the sale, you will pay a one-time gas fee at this point for initializing your account (I have this impression of being charged once for ETH and once for WETH, but I may be wrong). Prices, as mentioned, depend on the prevailing gas fee at that time, which you can check [here][12].\\n\\nThat's about it. \\n\\n*Repeat and rinse for more collections and more items in each collection.*\\n\\n### Marketing ###\\n\\nWhat we did above is probably the most technical (though not that technical), but also the simplest part of the entire process. \\n\\nThe hard part is getting the NFTs out there, and made known to people who are interested.\\n\\nI tried sharing the works on Reddit subreddits such as nft, Discord channels such as NFT Asia, and my own social media channels, but this part of the puzzle (which is probably the most important), is something I am still trying to explore. Until then, I think my OpenSea account is just another portfolio site for me.\\n\\nI suppose the NFT domain is not too different from the art domain. A friend once advised me to understand the artist's marketing capabilities over his artistic or technical capabilties if the intent was to invest in his or her art (rather than just admire it). I suppose this is equally true of NFTs and NFT creators.\\n\\n\\n~~~\\n\\n[1]:\\thttps://opensea.io/\\n[2]:    https://opensea.io/collection/whimsicalethereality\\n[3]:    https://opensea.io/collection/ethereal-attractors\\n[4]:    https://playgrd.com/art\\n[5]:    https://github.com/playgrdstar\\n[6]:    https://ethereum.org/en/nft/\\n[7]:    https://www.lifestyleasia.com/ind/gear/tech/top-nft-marketplaces/\\n[8]:    https://solanart.io/\\n[9]:    https://www.theverge.com/2022/2/2/22914081/open-sea-nft-marketplace-web3-fundraising-finzer-a16z?scrolla=5eb6d68b7fedc32c19ef33b4\\n[10]:   https://www.publish0x.com/freelance-ghostwriting/opensea-s-gas-free-nft-marketplace-on-polygon-xelelny?a=APdRorlKeG&tid=Twitter8.15\\n[11]:   https://www.coindesk.com/learn/polygon-and-matic-whats-the-difference/#:~:text=Polygon%20is%20a%20secondary%20scaling,it%20becomes%20ever%20more%20popular\\n[12]:   https://etherscan.io/gastracker\",\n",
       " \"---\\nlayout: default\\ntitle:  Measuring Market Risk in Python\\ndescription: Measuring Market Risk in Python\\ndate:   2021-01-26 00:00:00 +0000\\npermalink: /market_risk/\\ncategory: Finance\\n---\\n## Measuring Market Risk in Python\\n\\nVAR is a common term that one would usually come across in finance when it comes to the measurement of market risks.\\n\\nVAR, or Value At Risk, is basically a measure of the potential losses that one could face, at a specific level of confidence - e.g. 99%. \\n\\nBut before we get into VAR, we first need to discuss what value we are assessing risk against. What we want to measure would be the change in market prices over a time period (e.g. day to day). So what VAR would then tell us then would be how much we could lose (or gain) due to the change in prices. It's quite common to use lognormal instead of normal returns when computing the change in prices.\\n\\nUseful links which provide more information on the difference between the two -\\n\\n- https://quantivity.wordpress.com/2011/02/21/why-log-returns/\\n- http://www.insight-things.com/log-normal-distribution-mistaken\\n\\nEssentially, a few points to note -\\n- We assume prices are lognormal, then the log returns are normally distributed.\\n- When returns are small, it is hard to tell the difference between a lognormal and normal distribution\\n- Lognormal allows us, when compounding, to simply add returns (rather than multiplying). The sum of log returns then simply becomes the difference in the log of final and initial price.\\n\\nComputing relative or lognormal returns is fairly straight forward with the shift function in pandas. \\n\\n**Relative returns**\\n```\\nFX_Returns = (FX_DF/FX_DF.shift(1))-1\\n```\\n\\n**Lognormal returns**\\n```\\nFX_DF_LogReturns = np.log(FX_DF/FX_DF.shift(1))\\n```\\n\\nNow for **Value at Risk**.\\n\\nThere's nothing very complicated about Value at Risk (VAR). To put it simply, it's simply a single metric that shows the potential losses of a portfolio etc (at different confidence levels). There are two main methods to compute VAR -\\n\\n- Parametric\\n- Historical\\n\\nVery often, the parametric VAR is based on a normal distribution. Plotting a normal distribution and the VAR on a chart will give us a good overview of how this works.\\n```\\n# We use z to define how many standard deviations away from the mean\\n# Here we use z = -2.33 to get to a 99% confidence interval. Why 99% will be obvious once we plot out the distribution\\nz = -2.33\\n\\nplt.figure(figsize=(12,8))\\n# plotting the normal distribution, using the scipy stats norm function\\nplt.ylim(0, 0.5)\\nx = np.arange(-5,5,0.1)\\ny1 = stats.norm.pdf(x)\\nplt.plot(x, y1)\\n\\nx2 = np.arange(-5, z, 0.01) # we use this range from the -ve end to -2.33 to compute the area\\nsum = 0\\n# s = np.arange(-10,z,0.01)\\nfor i in x2:\\n    sum+=stats.norm.pdf(i)*0.01 # computing area under graph from -5 to -2.33 in steps of 0.01\\n\\nplt.annotate('Area is ' + str(round(sum,3)), xy = (z-1.3, 0.05), fontsize=12)\\nplt.annotate('z=' + str(z), xy=(z, 0.01), fontsize=12)\\nplt.fill_between(x2, stats.norm.pdf(x2))\\nplt.show()\\n```\\n\\nComputing parametric VAR is fairly straightforward.\\n```\\nconfidence = 0.99\\nZ = stats.norm.ppf(1-confidence)\\nmean = np.mean(EQ_Returns)\\nstddev = np.std(EQ_Returns)\\n\\nParam_VAR = latest_price*Z*stddev\\n```\\n\\nAnd so is computing historical VAR.\\n```\\nHist_VAR = latest_price*np.percentile(EQ_Returns, 1))\\n```\\n\\nI've simplified some of the code in this post for ease of understanding, but the full code can be found at the notebook [here][1].\\n\\n[1]:\\thttps://github.com/playgrdstar/measure_marketrisk_VAR/blob/master/Introduction%20-%20Measuring%20Market%20Risk%20in%20Python.ipynb\",\n",
       " \"And since I will almost certainly forget what went through my mind during this process, I decided to jot down some notes on key points that I learnt or went through my mind. \\n\\nI'm not very hopeful about selling any of my NFTs, given how crowded the marketplaces have become in the short span of a year or so, so this post would probably be my main takeaway.\\n\\nSome of these points could potentially be wrong, or outdated. If so, please let me know.\",\n",
       " \"![Model](/assets/media/hamp_simple.png) \\n\\nThe main inspiration for this paper was an interesting [work][3] by Geoffrey Hinton that was published a year ago. The line in Hinton's paper that motivated my paper is the first line in the introduction – on how people break down visual scenes into part-whole hierarchies when understanding the visual scene.\\n\\n![Hinton's Paper](/assets/media/glom.png) \\n\\nBasically, it got me thinking about how UI designs can be naturally viewed as part-whole hierarchies. More importantly, it is how the parts fit within the wholes that determine the role of different UI objects and influence how people understand and interact with UIs.\\n\\nAnd if you take this line of thought further, it is equally applicable to any type of design or even artwork.\\n\\n\\n[1]:\\thttps://dl.acm.org/doi/10.1145/3397481.3450693\\n[2]:\\thttps://dl.acm.org/doi/10.1145/3490099.3511143\\n[3]:    https://arxiv.org/abs/2102.12627\",\n",
       " 'async function train(numIterations, done){\\n    \\n    const d = await fetchData();\\n\\n    for (let i=0; i<numIterations; i++){\\n        let cost;\\n        const [xs,ys] = await batchData(d.x,d.y,50);\\n        cost = tf.tidy(()=>{\\n            cost = optimizer.minimize(()=>{\\n                const pred = predict(xs);\\n                const predLoss = loss(pred, ys);\\n\\n                return predLoss;\\n                \\n            }, true);\\n            return cost;\\n        })\\n\\n        cost.data().then((data)=>lossArray.push({i:i, error:data[0]}));\\n        \\n        if (i%100==0){\\n            ploterrors(lossArray.slice(i-100,i));\\n            await cost.data().then((data)=>console.log(i,data));\\n        }\\n\\n        await tf.nextFrame();\\n    }\\n    done();\\n    // If we want to check the values of A and b\\n    // await A.data().then((data)=>console.log(data[0]));\\n    // await b.data().then((data)=>console.log(data[0]));\\n}\\n```\\n\\nIf you are familiar with Tensorflow in Python, this should be fairly easy to understand. If not, do take a look at the Tensorflow documentation. The key difference here is that we don’t use Sessions, but rather declare predict, loss etc as functions, that are then called later in train with the data to be used for training.\\n\\nWe also add `await tf.nextFrame();` here. What this does is basically make sure that the browser screen continues to render even as training goes on.   \\n\\nIt’s also important to understand what `tf.tidy()` is for. Whenever we use a tensor, it takes up space in our GPU. We can call `dispose` after each step to clear the space, but that would be really tedious. What `tf.tidy()` does is that it helps us clear up all the intermediate tensors.\\n\\nThere’s also something that puzzles me. In tensorflow.js, we can do operations either by `tf.add(a,b)` or `a.add(b)`. But somehow, only the latter works within the `optimizer.minimize` function. If anyone knows why, let me know?\\n\\nThe full code is [here][2].\\n\\nSubscribe to more of such posts [here][3]\\n\\n\\n[1]:\\thttps://playgrdstar.github.io/tensorflow_browser_d3/\\n[2]:\\thttps://github.com/playgrdstar/tensorflow_browser_d3\\n[3]:\\thttps://www.subscribepage.com/f5e3a3']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensemble_retriever(query, all_retrievers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Long Context Re-ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Effect of this function is to alternate elements from the ends towards the center when looking at the original order of docs\n",
    "def reorder_docs(docs):\n",
    "    docs.reverse()\n",
    "    reordered_result = []\n",
    "    for i, value in enumerate(docs):\n",
    "        if i % 2 == 1:\n",
    "            reordered_result.append(value) # append even numbered elements\n",
    "        else:\n",
    "            reordered_result.insert(0, value) # add odd numbered elemenats to the start\n",
    "    return reordered_result\n",
    "                 \n",
    "retrieved_docs = simple_retriever.get_relevant_documents(query)\n",
    "reordered_docs = reorder_docs(docs)\n",
    "\n",
    "print(get_text(reordered_docs)[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def long_context_reorder_retrieval(query, retriever):\n",
    "    retrieved_docs = retriever.get_relevant_documents(query)\n",
    "    retrieved_docs.reverse()\n",
    "    reordered_results = []\n",
    "    for i, value in enumerate(retrieved_docs):\n",
    "        if i % 2 == 1:\n",
    "            reordered_results.append(value) # append even numbered elements\n",
    "        else:\n",
    "            reordered_results.insert(0, value) # add odd numbered elemenats to the start\n",
    "    return get_text(reordered_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['---\\nlayout: default\\ntitle:  How normal are you? Checking distributional assumptions.\\ndescription: How normal are you? Checking distributional assumptions.\\ndate:   2021-01-27 00:00:00 +0000\\npermalink: /normality_distribution_test/\\ncategory: Finance\\n---\\n## How normal are you? Checking distributional assumptions.\\n\\nThe need to understand the underlying distribution of data is critical in most parts of quantitative finance. Statistical tests can be applied for this purpose.',\n",
       " \"And since I will almost certainly forget what went through my mind during this process, I decided to jot down some notes on key points that I learnt or went through my mind. \\n\\nI'm not very hopeful about selling any of my NFTs, given how crowded the marketplaces have become in the short span of a year or so, so this post would probably be my main takeaway.\\n\\nSome of these points could potentially be wrong, or outdated. If so, please let me know.\",\n",
       " '---\\nlayout: default\\ntitle:  Monte Carlo Simulation of Value at Risk in Python\\ndescription: Monte Carlo Simulation of Value at Risk in Python\\ndate:   2021-01-26 00:00:02 +0000\\npermalink: /monte_carlo_var/\\ncategory: Finance\\n---\\n## Monte Carlo Simulation of Value at Risk in Python',\n",
       " '- Dec. 2021: \"Learning User Interface Semantics from Heterogeneous Networks with Multimodal and Positional Attributes\" accepted by 27th ACM International Conference on Intelligent User Interfaces (IUI 2022)\\n- Oct. 2021: \"Learning Knowledge-Enriched Company Embeddings for Investment Management\" accepted by the 2nd ACM International Conference on AI in Finance (ICAIF 2021)',\n",
       " '[1]:\\thttps://medium.com/quaintitative/data-exploration-in-pandas-f7cd1a3b3594\\n[2]:\\thttps://medium.com/quaintitative/quickstart-to-visualising-and-analysing-financial-data-with-pandas-bbd835c9c560\\n[3]:\\thttps://playgrdstar.github.io/portfolio_optimisation_with_tensorflow/\\n[4]:\\thttps://medium.com/creative-coding-space/3-days-of-hand-coding-visualisations-introduction-64da30d8793f\\n[5]:\\thttps://github.com/playgrdstar/portfolio_optimisation_with_tensorflow']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_context_reorder_retrieval(query, simple_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
